ğŸ“„ Document Chat â€“ RAG Based Application

Overview

This project is a Streamlit-based Retrieval-Augmented Generation (RAG) application that allows users to upload any PDF, DOCX and TXT files and chat with it.

The system extracts text from the uploaded files, chunks it, generates embeddings, stores them in a vector database, retrieves relevant context, and generates grounded answers with citations.

The app supports follow-up questions using persistent chat history.

ğŸ— Architecture Flow

PDF Upload â†’ Text Extraction â†’ Chunking â†’ Embeddings â†’ Vector Store â†’ Retrieval â†’ Grounded Answer with Citations

ğŸ§  Tech Stack

1) Component	Technology Used

1. UI:	Streamlit

2. LLM:	Gemini 2.5 Flash

3. Embeddings:	gemini-embedding-001 (3072 dimensions)

4. Vector DB:	Qdrant (local mode)

5. Retrieval:	Cosine Similarity Top-K

6. Language:	Python 3.10+


Processing Pipeline

1ï¸âƒ£ Document Upload

-- User uploads a single file via Streamlit.

-- File is processed in memory.

2ï¸âƒ£ Text Extraction

-- Uses Python based extraction.

-- Preserves page numbers for citation support.

-- Each chunk retains: Text & Page number

3ï¸âƒ£ Chunking Strategy

-- Chunk size: ~800 characters

-- Overlap: ~100 characters

-- Strategy: Sliding window

Reason: Maintains semantic continuity and Reduces context loss

-- Optimized for embedding model token limits

4ï¸âƒ£ Embeddings

-- Model: gemini-embedding-001

-- Dimension: 3072

Why: 

-- High-quality semantic embeddings

-- Native compatibility with Gemini ecosystem

-- Strong retrieval performance

5ï¸âƒ£ Vector Store

-- Database: Qdrant (Local Mode)

-- Collection initialized dynamically

-- Vector size = 3072

-- Distance metric = Cosine Similarity

-- Points include metadata:

-- page number & text snippet

-- session id

Reason: Lightweight, Fast local development & Production-ready scalable architecture

6ï¸âƒ£ Retrieval Strategy

-- Top-K = 5

-- Cosine similarity search

-- Filter by session_id to isolate documents per chat

Why:

-- Ensures only relevant document chunks are retrieved

-- Supports multiple chat sessions

7ï¸âƒ£ RAG (Q&A Generation)

Prompt includes:

-- Retrieved context chunks

-- Page numbers

-- User question

-- Chat history (for follow-ups)

Rules enforced:

-- Answers must be grounded in context

-- Must include page citations

-- If information not found â†’ respond accordingly

Features

âœ… Upload any single File

âœ… Chat interface

âœ… Persistent conversation history

âœ… Follow-up question support

âœ… Page citations in answers

âœ… Latency breakdown (Embedding / Retrieval / Generation)

âœ… Multi-chat sidebar (ChatGPT style)

âœ… Automatic chat renaming

âœ… Evaluation table (5-question test)

ğŸ§ª Evaluation (Basic Accuracy Test)

The app includes 5 test sample questions:

Question	Expected Keywords	Result

Total experience?	2+ years	âœ…

Churn models used?	Logistic, Random Forest, XGBoost	âœ…

Election forecasting accuracy?	92%	âœ…

Web scraping tools?	Beautiful Soup, Selenium	âœ…

Skill extraction model?	BERT	âœ…

Accuracy: 100% on internal test document

ğŸš€ How to Run Locally

1ï¸âƒ£ Clone the repository

git clone <your-repo-url>

cd pdf-rag-app

2ï¸âƒ£ Install dependencies

pip install -r requirements.txt

3ï¸âƒ£ Set Environment Variable

Create a .env file:

GEMINI_API_KEY=your_api_key_here

OR export directly:

export GEMINI_API_KEY=your_key   # Mac/Linux

setx GEMINI_API_KEY your_key     # Windows

4ï¸âƒ£ Run the app

streamlit run app.py

App will open at:

http://localhost:8501

ğŸ“‚ Project Structure
pdf-rag-app/

â”‚

â”œâ”€â”€ app.py

â”œâ”€â”€ requirements.txt

â”œâ”€â”€ README.md

â”‚

â””â”€â”€ core/

    â”œâ”€â”€ document_loader.py
    
    â”œâ”€â”€ chunker.py
    
    â”œâ”€â”€ embeddings.py
    
    â”œâ”€â”€ vectorstore.py
    
    â”œâ”€â”€ retriever.py
    
    â”œâ”€â”€ rag_pipeline.py
    
    â””â”€â”€ prompt.py

âœ… Requirements Coverage

Upload any file	âœ…

Persistent chat	âœ…

Show citations	âœ…

Sensible chunking	âœ…

Mention embedding model + dimension	âœ…

Vector DB used	âœ…

Retrieval strategy explained	âœ…

Grounded RAG prompt	âœ…

Follow-up handling	âœ…

streamlit run works	âœ…

Modular code	âœ…

Basic evaluation	âœ…

Edge Case Handling

-- Re-uploading same document skips re-indexing

-- Handles empty retrieval results

-- Prevents cross-session contamination

-- Avoids crashes on missing context

-- Graceful quota handling
